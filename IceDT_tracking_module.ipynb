{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Version 13.09.21 - Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "sys.path.append('./libs')\n",
    "\n",
    "import os\n",
    "os.environ[\"PROJ_LIB\"] = r\"~/anaconda3/Library/share\"; #fixr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import difflib\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import getMetadata as gmeta\n",
    "import gc\n",
    "import myGeoTools as mgt\n",
    "import tracks_modules as tm\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "from fastdtw import fastdtw\n",
    "from scipy import stats\n",
    "from geopy.distance import geodesic as geodist\n",
    "from similarityMeasures import Similarity\n",
    "from datetime import date\n",
    "from IPython.display import clear_output # progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path folder settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP= r'./'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = EXP+r'entradas/'\n",
    "file = 'out_tracks_Weddel.txt'\n",
    "path_out = EXP+r'saidas/'\n",
    "\n",
    "path_track = path_in + file\n",
    "\n",
    "measures = Similarity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppening iceberg Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "detected_icebergs = pd.read_csv(path_track, sep=' ')\n",
    "\n",
    "rows_to_drop = detected_icebergs.loc[detected_icebergs['date']=='date'].index\n",
    "\n",
    "detected_icebergs.drop(rows_to_drop, inplace=True)\n",
    "\n",
    "detected_icebergs['date'] = pd.to_datetime(detected_icebergs['date'].astype(str), format='%Y-%m-%d')\n",
    "detected_icebergs['minoraxis_px'] = detected_icebergs['minoraxis_px'].astype('int')\n",
    "detected_icebergs['majoraxis_px'] = detected_icebergs['majoraxis_px'].astype('int')\n",
    "detected_icebergs['perimeter_px'] = detected_icebergs['perimeter_px'].astype('int')\n",
    "detected_icebergs['area_px'] = detected_icebergs['area_px'].astype('int')\n",
    "\n",
    "detected_icebergs['latitude'] = detected_icebergs['latitude'].astype('float')\n",
    "detected_icebergs['longitude'] = detected_icebergs['longitude'].astype('float')\n",
    "\n",
    "detected_icebergs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert pixel to Km and add to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "pixel_size = 75.\n",
    "detected_icebergs['minoraxis_km'] = round((detected_icebergs['minoraxis_px'] * pixel_size) * 1e-3, 3)\n",
    "detected_icebergs['majoraxis_km'] = round((detected_icebergs['majoraxis_px'] * pixel_size) * 1e-3, 3)\n",
    "detected_icebergs['perimeter_km'] = round((detected_icebergs['perimeter_px'] * pixel_size) * 1e-3, 3)\n",
    "detected_icebergs['area_km2'] = round((detected_icebergs['area_px'] * (pixel_size**2)) * 1e-6, 3)\n",
    "#detected_icebergs['mass_gt'] = round((((detected_icebergs['area_km2'] * 1e6) * 250) * 850) * 1e-12, 3) \n",
    "\n",
    "#used_col = detected_icebergs.pop('used')\n",
    "#detected_icebergs['used'] = used_col\n",
    "n_total_detections = len(detected_icebergs['date'])\n",
    "n_total_detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = 1.\n",
    "max_size = 4000\n",
    "#detected_icebergs = detected_icebergs[(detected_icebergs['area_km2'] >= min_size) & (detected_icebergs['area_km2'] < max_size)].reset_index()\n",
    "detected_icebergs = detected_icebergs[(detected_icebergs['area_km2'] >= min_size) & (detected_icebergs['area_km2'] < max_size)]\n",
    "\n",
    "#detected_icebergs = detected_icebergs.sort_values(by=['date', 'area_km2'], ascending=[True, False])\n",
    "\n",
    "detected_icebergs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ices = len(detected_icebergs['date'])\n",
    "print('Total of icebergs detections: ', n_total_detections)\n",
    "print('Total selected by size: ', num_ices)\n",
    "print('Bigger area: ', detected_icebergs['area_km2'].max())\n",
    "print('Smaller area: ', detected_icebergs['area_km2'].min())\n",
    "print('========================================')\n",
    "\n",
    "detected_icebergs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "#plt.figure(figsize=(10,8))\n",
    "data = np.asarray(detected_icebergs['area_km2'].values)\n",
    "n, bins, _ = plt.hist(data, bins=40, density=True, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "plt.vlines(np.nanmean(data), ymin=0, ymax=np.max(n), color='red', label='Average')\n",
    "\n",
    "#plt.title(\"histogram\")\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Area km$^{2}$')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "#plt.savefig('Area_distribution.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_heuristics(delta_km, delta_days, area, speed, pairs=False):\n",
    "    \n",
    "    speed_tsh = 0.\n",
    "    days_tsh = 0.\n",
    "    \n",
    "    if 0. < area <= 1.:\n",
    "        radius = 9. * delta_days\n",
    "        speed_tsh = 6.5 * 2\n",
    "        days_tsh = 25\n",
    "        \n",
    "    if 1. < area <= 10.:\n",
    "        radius = 6.5 * delta_days #* 2 7.5\n",
    "        speed_tsh = 6.5 * 2\n",
    "        days_tsh = 40\n",
    "        \n",
    "    if 10. < area <= 100.:\n",
    "        radius = 4 * delta_days #* 2 5.5\n",
    "        speed_tsh = 4. * 2\n",
    "        days_tsh = 60\n",
    "        \n",
    "    if 100. < area <= 1000.:\n",
    "        radius = 2.3 * delta_days * 1.5\n",
    "        speed_tsh = 2.3 * 2\n",
    "        days_tsh = 70 if area < 500 else 120\n",
    "        \n",
    "    if area > 1000.:\n",
    "        radius = 2.5 * delta_days * 2 # 2.5\n",
    "        speed_tsh = 2.32 * 2\n",
    "        days_tsh = 90 if area < 1500 else 180\n",
    "        \n",
    "    #is_dist_max = True if delta_km < max_dist else False #550\n",
    "    is_dist_max = True\n",
    "    is_radius = True if delta_km <= radius else False\n",
    "    is_speed = True if speed <= speed_tsh else False\n",
    "    is_days = True if delta_days <= days_tsh else False\n",
    "    \n",
    "    return [is_dist_max, is_radius, is_speed, is_days]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w):\n",
    "    return (np.convolve(x, np.ones(w), 'valid') / w) ##.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean possible iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_track(iceberg_to_clean, berg_individual_id, main_bar, smooth_len):\n",
    "\n",
    "    save_base = iceberg_to_clean.copy()\n",
    "    \n",
    "    total_samples = len(iceberg_to_clean.index)\n",
    "    \n",
    "    last_id_valid = iceberg_to_clean.first_valid_index()\n",
    "    last_id_valid_check = -1\n",
    "    \n",
    "    indexes_used = []\n",
    "    \n",
    "    while len(iceberg_to_clean.index) > 1:\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        bar_length=20\n",
    "        progress = float(total_samples-len(iceberg_to_clean.index))/(total_samples)\n",
    "        block = int(round(bar_length * progress))\n",
    "        progressbarmain = \"Global progress: [{0}] {1:.1f}%\".format( \"#\" * int(main_bar[0]) + \"-\" * (bar_length - int(main_bar[0])), main_bar[1] * 100.)\n",
    "        progressbar = \"Filtering progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100.)\n",
    "        print(progressbarmain)\n",
    "        print(progressbar, 'Total detections for current track: ', len(indexes_used))\n",
    "        \n",
    "        if last_id_valid != last_id_valid_check:\n",
    "            \n",
    "            area_base = iceberg_to_clean.loc[last_id_valid,'area_km2']\n",
    "            \n",
    "            iceberg_base_signature = moving_average(np.sort(np.asarray(json.loads(iceberg_to_clean.loc[last_id_valid,'shape']))).tolist(), smooth_len)\n",
    "\n",
    "            iceberg_to_clean['jcd_to_base'] = iceberg_to_clean.apply(lambda row : measures.jaccard_similarity(iceberg_base_signature, moving_average(np.sort(np.asarray(json.loads(row['shape']))).tolist(),smooth_len)), axis = 1)\n",
    "            iceberg_to_clean['k_s'] = iceberg_to_clean.apply(lambda row : 1 - stats.ks_2samp(iceberg_base_signature, moving_average(np.sort(np.asarray(json.loads(row['shape']))).tolist(),smooth_len))[0], axis = 1)\n",
    "            #iceberg_to_clean['vonmisses'] = iceberg_to_clean.apply(lambda row : 1 - stats.cramervonmises_2samp(iceberg_base_signature, moving_average(np.sort(np.asarray(json.loads(row['shape']))).tolist(),smooth_len)).statistic, axis = 1)\n",
    "            #iceberg_to_clean.loc[iceberg_to_clean['vonmisses'] < 0, 'vonmisses'] = 0\n",
    "            \n",
    "            iceberg_to_clean['sim_score'] = (iceberg_to_clean['jcd_to_base'] + iceberg_to_clean['k_s'])/2\n",
    "            iceberg_to_clean = iceberg_to_clean.sort_values(by=['sim_score'], ascending=[False])\n",
    "                \n",
    "            if area_base < 1000:\n",
    "                sim_tsh = 0.9\n",
    "                sim_id = 1\n",
    "            else:\n",
    "                sim_tsh = 0.85\n",
    "                sim_id = 1\n",
    "        \n",
    "        next_id = iceberg_to_clean.index[1]\n",
    "        \n",
    "        sims = [iceberg_to_clean.loc[next_id,'jcd_to_base'],\n",
    "                iceberg_to_clean.loc[next_id,'k_s'],\n",
    "                iceberg_to_clean.loc[next_id,'sim_score']]\n",
    "             \n",
    "        #if sim_pair_k_s > sim_tsh: #0.85\n",
    "        if sims[sim_id] > sim_tsh:\n",
    "                        \n",
    "            indexes_used.append(last_id_valid)\n",
    "            iceberg_to_clean.drop(last_id_valid, inplace=True)\n",
    "            last_id_valid_check = last_id_valid\n",
    "            last_id_valid = next_id\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            iceberg_to_clean.drop(next_id, inplace=True)\n",
    "            last_id_valid_check = last_id_valid\n",
    "            \n",
    "    \n",
    "    if len(indexes_used) >= 5:\n",
    "            \n",
    "        berg_to_save = save_base.loc[indexes_used, :].sort_values(by=['date'], ascending=[True])\n",
    "        berg_to_look = berg_to_save.copy()\n",
    "        \n",
    "        indexes_used_final = []\n",
    "        while len(berg_to_look.index) > 0:\n",
    "        \n",
    "            indexes_used = berg_to_look.index.tolist()\n",
    "            \n",
    "            idx_start_look = berg_to_look.first_valid_index()\n",
    "            idx_ant = idx_start_look\n",
    "            row_ant = berg_to_look.loc[idx_ant, :]\n",
    "            \n",
    "            for index, row in berg_to_save.loc[idx_ant:, :].iterrows():\n",
    "\n",
    "                droped = False\n",
    "                if idx_ant != index:\n",
    "\n",
    "                    dkm = round(geodist((row_ant['latitude'], row_ant['longitude']), (row['latitude'], row['longitude'])).km, 3)\n",
    "                    ddays = abs((row['date'] - row_ant['date']).days)\n",
    "\n",
    "                    ddays = 1. if ddays == 0. else ddays\n",
    "                    dspeed = round(dkm/ddays, 3)\n",
    "\n",
    "                    area_change = abs(row['area_km2'] - row_ant['area_km2'])\n",
    "                    area_dst = area_change / row_ant['area_km2']\n",
    "\n",
    "                    heuristics = check_heuristics(dkm, ddays, row_ant['area_km2'], dspeed, pairs=True)\n",
    "\n",
    "                    #if not all(heuristics):\n",
    "                    if not heuristics[1] or not heuristics[3] or not area_dst < 0.21 :\n",
    "                        droped = True\n",
    "                        indexes_used.pop(indexes_used.index(index))\n",
    "\n",
    "                if not droped: \n",
    "                    row_ant = row\n",
    "                    idx_ant = index\n",
    "                    \n",
    "            if len(indexes_used) > len(indexes_used_final):\n",
    "                indexes_used_final = indexes_used\n",
    "                \n",
    "            berg_to_look.drop(idx_start_look, inplace=True)\n",
    "\n",
    "            \n",
    "        berg_to_look = berg_to_save.loc[indexes_used_final]\n",
    "        if berg_to_look.shape[0] >= 3:\n",
    "            berg_to_look.to_csv('./saidas/track_'+str(berg_individual_id)+'_'+str(indexes_used_final[0])+'.txt', mode = 'w', columns = ['date','latitude','longitude','majoraxis_km', 'area_km2'], sep=' ', index=False)\n",
    "            \n",
    "    else:\n",
    "        indexes_used_final = indexes_used\n",
    "        \n",
    "    return indexes_used_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop tracking procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.process_time()\n",
    "\n",
    "iceberg_control = detected_icebergs.copy().sort_values(by='date', ascending=True)\n",
    "\n",
    "idx = iceberg_control.first_valid_index()\n",
    "\n",
    "#delta_years_ths = 10\n",
    "#smooth_len = 3\n",
    "\n",
    "while len(iceberg_control.index) > 0:\n",
    "\n",
    "    progressbar = 0\n",
    "    indexes = iceberg_control.index\n",
    "    idx = iceberg_control.first_valid_index()\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    if idx in indexes:\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        bar_length=20\n",
    "        progress = float(num_ices-len(indexes))/(num_ices)\n",
    "        block = int(round(bar_length * progress))\n",
    "        progressbar = \"Global progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100.)\n",
    "        print(progressbar)\n",
    "        \n",
    "        main_bar = [block, progress]\n",
    "        \n",
    "        iceberg_base = iceberg_control.loc[idx, :]\n",
    "\n",
    "        date_iceberg_base = iceberg_base['date']\n",
    "        area_iceberg_base = iceberg_base['area_km2']\n",
    "        \n",
    "        if area_iceberg_base > 50 :\n",
    "            smooth_len = 3\n",
    "        else:\n",
    "            smooth_len = 2\n",
    "            \n",
    "        if area_iceberg_base > 100:\n",
    "            delta_years_ths = 10\n",
    "        else:\n",
    "            delta_years_ths = 5\n",
    "        \n",
    "        iceberg_base_morpho_signature = moving_average(np.sort(np.asarray(json.loads(iceberg_base['shape']))).astype(int).tolist(), smooth_len)\n",
    "        \n",
    "        c2 = iceberg_control['longitude'] > -65 ## APENAS PARA WEDDELL\n",
    "\n",
    "        possible_icebergs_df = iceberg_control.loc[c2]\n",
    "        possible_icebergs_df = possible_icebergs_df[((possible_icebergs_df['date'] - iceberg_base['date']).dt.days)/365 <= delta_years_ths]\n",
    "        \n",
    "        if not possible_icebergs_df.empty:\n",
    "        \n",
    "            possible_icebergs_df['jcd_to_base'] = possible_icebergs_df.apply(lambda row : measures.jaccard_similarity(iceberg_base_morpho_signature, moving_average(np.sort(np.asarray(json.loads(row['shape']))).tolist(),smooth_len)), axis = 1)\n",
    "            possible_icebergs_df['k_s'] = possible_icebergs_df.apply(lambda row : 1 - stats.ks_2samp(iceberg_base_morpho_signature, moving_average(np.sort(np.asarray(json.loads(row['shape']))).tolist(),smooth_len))[0], axis = 1)\n",
    "            #possible_icebergs_df['vonmisses'] = possible_icebergs_df.apply(lambda row : 1 - stats.cramervonmises_2samp(iceberg_base_morpho_signature, moving_average(np.sort(np.asarray(json.loads(row['shape']))).tolist(),smooth_len)).statistic, axis = 1)\n",
    "            #possible_icebergs_df.loc[possible_icebergs_df['vonmisses'] < 0, 'vonmisses'] = 0\n",
    "            \n",
    "            possible_icebergs_df['sim_score'] = (possible_icebergs_df['jcd_to_base'] + possible_icebergs_df['k_s'])/2\n",
    "\n",
    "            possible_individual_iceberg_df = possible_icebergs_df[(possible_icebergs_df['sim_score'] > 0.5)].sort_values(by='sim_score', ascending=False) #0.5\n",
    "            \n",
    "            if not len(possible_individual_iceberg_df['jcd_to_base']) == 0:\n",
    "\n",
    "                index_to_drop = clean_track(possible_individual_iceberg_df, idx, main_bar, smooth_len)\n",
    "                \n",
    "                if index_to_drop == []:\n",
    "                    index_to_drop = idx\n",
    "\n",
    "            else:\n",
    "                index_to_drop = idx\n",
    "\n",
    "            iceberg_control.drop(index_to_drop, inplace=True)\n",
    "        \n",
    "        else:\n",
    "            iceberg_control.drop(indexes, inplace=True)            \n",
    "    \n",
    "clear_output(wait=True)\n",
    "bar_length=20\n",
    "progress = float(num_ices)/(num_ices)\n",
    "block = int(round(bar_length * progress))\n",
    "progressbar = \"Global progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100.)\n",
    "print(progressbar)\n",
    "\n",
    "t1 = (time.process_time() - t0) / 60.\n",
    "print(\"Total time elapsed: \", round(t1, 3), 'minutes') # CPU seconds elapsed (floating point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trackingenv",
   "language": "python",
   "name": "trackingenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
